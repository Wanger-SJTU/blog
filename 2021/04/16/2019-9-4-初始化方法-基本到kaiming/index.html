

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/favicon.png">
  <link rel="icon" href="/blog/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="wanger">
  <meta name="keywords" content="">
  
  <title>初始化方法-基本到kaiming - Wang Er</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/blog/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"wanger-sjtu.github.io","root":"/blog/","version":"1.8.9a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/blog/">&nbsp;<strong>Fluid</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/blog/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="初始化方法-基本到kaiming">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-04-16 11:20" pubdate>
        2021年4月16日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      1.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      26
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">初始化方法-基本到kaiming</h1>
            
            <div class="markdown-body">
              <h3 id="为什么需要初始化"><a href="#为什么需要初始化" class="headerlink" title="为什么需要初始化"></a>为什么需要初始化</h3><p>初始化的原因，</p>
<ul>
<li>防止每一层的输出太大或者太小，导致梯度反向传播过程中，梯度爆炸或者梯度消失。</li>
<li>不能采用统一值得原因，因为统一值得初始化会使得每一层网络在不同通道学到得特征相同。</li>
</ul>
<p>上述原因都会导致，网络模型不能收敛。</p>
<h4 id="简单例子得说明"><a href="#简单例子得说明" class="headerlink" title="简单例子得说明"></a>简单例子得说明</h4><p>假如我们有一个输入<code>x</code> ，定义为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="hljs-number">512</span>)<br></code></pre></td></tr></table></figure>
<p><code>x</code>是`均值为 $0$，方差是 $1$ 的高斯分布。然后定义一个100层的神经网络（注：不包含激活函数）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x = a @ x<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br></code></pre></td></tr></table></figure>
<p>那么得到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(tensor(nan),tensor(nan))<br></code></pre></td></tr></table></figure>
<p>输出已经是无穷大了。通过下面的代码，可以知道大概29层以后，输出就已经无法计算了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x = a @ x<br>    <span class="hljs-keyword">if</span> torch.isnan(x.std()):<br>        <span class="hljs-keyword">break</span><br><span class="hljs-built_in">print</span>(i) <span class="hljs-comment"># 28</span><br></code></pre></td></tr></table></figure>
<p>既然输出太大，我们把神经网络的初始化变小一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)*<span class="hljs-number">0.01</span><br>    x = a @ x<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment"># 0, 0</span><br></code></pre></td></tr></table></figure>
<p>那么得到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(tensor(<span class="hljs-number">0.</span>),tensor(<span class="hljs-number">0.</span>))<br></code></pre></td></tr></table></figure>
<p>这时候的输出就太小，没办法计算了。</p>
<h3 id="怎么找到合适的初始化方法"><a href="#怎么找到合适的初始化方法" class="headerlink" title="怎么找到合适的初始化方法"></a>怎么找到合适的初始化方法</h3><p>对于神经网络来说，前向传播过程就是矩阵运算，假设一层的输出为$y$</p>
<script type="math/tex; mode=display">
y_i= \sum_{k=1}^{n-1}a_{i,k}x_k</script><p>$i$ 是矩阵 $\mathbf{m}$ 的行，$k$ 是矩阵 $\mathbf{m}$ 的列。python的计算代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">y[i] = <span class="hljs-built_in">sum</span>([c*d <span class="hljs-keyword">for</span> c,d <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(a[i], x)])<br></code></pre></td></tr></table></figure>
<p>可以证明，在给定的层，从标准正态分布初始化的输入$x$ 和权重矩阵 $a$ 的矩阵乘积平均具有非常接近输入<strong>连接数的平方根的标准偏差</strong>，在例子中是$\sqrt{512}$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">mean,var=<span class="hljs-number">0.</span>,<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    x = torch.randn(<span class="hljs-number">512</span>)<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    y = a @ x<br>    mean += y.mean().item()<br>    var += y.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean().item()<br><span class="hljs-built_in">print</span>(mean()/<span class="hljs-number">10000</span>, math.sqrt(var/<span class="hljs-number">10000</span>))<br><span class="hljs-comment">#0.00889449315816164  22.629779825053976</span><br><span class="hljs-built_in">print</span>(math.sqrt(<span class="hljs-number">512</span>))<br><span class="hljs-comment"># 22.627416997969522</span><br><br>mean,var = <span class="hljs-number">0.</span>,<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    x = torch.randn(<span class="hljs-number">512</span>)<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    b = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    y = a @ x<br>    z = b @ y<br>    mean += z.mean().item()<br>    var += z.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean().item()<br><span class="hljs-built_in">print</span>(mean/<span class="hljs-number">10000</span>, math.sqrt(var/<span class="hljs-number">10000</span>))<br><span class="hljs-comment">#0.6010947234869003 511.8684602024235</span><br></code></pre></td></tr></table></figure>
<p>如果我们根据如何定义矩阵乘法来看前向传播的过程：</p>
<p>为了计算 $y$，我们将输入 $x$ 的一个元素的乘以矩阵 $\mathbf{a}$ 的一列的512个乘积然后相加。在使用标准正态分布初始化$x$ 和 $a$ 的示例中，这$512$ 个数字中的每一个的平均值为 $0$，标准差为$1$。</p>
<blockquote>
<p><strong>经过一层网络运算以后，均值没变，方差扩大了$\sqrt{512}$倍。</strong></p>
</blockquote>
<p>因此在初始化的是，缩小$\sqrt{512}$倍，那么输出结果就能保证不<strong>爆炸</strong>了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">mean,var=<span class="hljs-number">0.</span>,<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    x = torch.randn(<span class="hljs-number">512</span>)<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    y = a @ x<br>    mean += y.mean().item()<br>    var += y.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean().item()<br><span class="hljs-built_in">print</span>(mean/<span class="hljs-number">10000</span>, math.sqrt(var/<span class="hljs-number">10000</span>))<br><span class="hljs-comment">#0.00039810733370250094 1.0007971983717594</span><br><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x = a @ x<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(-0.0048) tensor(1.2810)</span><br></code></pre></td></tr></table></figure>
<h3 id="Xavier-Initialization"><a href="#Xavier-Initialization" class="headerlink" title="Xavier Initialization"></a>Xavier Initialization</h3><p>上面介绍的情况是在不含有激活的函数情形，如果增加了激活函数，是否仍能保持不变呢？对于不同类型的激活函数，是不是有不同的表现呢？最开始用的激活函数多数为对称的，并且导数从中间到两边有递减为0。比如，常用的<code>tanh</code>和<code>sigmoid</code>函数。</p>
<p>下面的结果是在上面的例子中分别增加了，<code>tanh</code>和<code>sigmoid</code>函数的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#sigmoid</span><br>x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x = torch.sigmoid( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(0.5057) tensor(0.1180)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#tanh</span><br>x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x =  torch.tanh( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(-0.0051) tensor(0.0879)</span><br></code></pre></td></tr></table></figure>
<p>可以看到经过激活函数以后，函数方差明显变小了。在训练过程中这就会导致，导致梯度过小，使得训练难以进行。</p>
<p>上面用的是正态分布，如果采用均匀分布呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.Tensor(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>).uniform_(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x =  torch.tanh( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(-3.8077e-26) tensor(1.2476e-24)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.Tensor(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>).uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x =  torch.tanh( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(-1.) tensor(0.)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.Tensor(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>).uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x =  torch.sigmoid( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(1.0000) tensor(3.8114e-06)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.Tensor(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>).uniform_(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x =  torch.sigmoid( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(0.4934) tensor(0.0659)</span><br></code></pre></td></tr></table></figure>
<p>方差都出人意料的小。这就几乎不能学习到什么有用的特征了。</p>
<p>为此，Glorot and Bengio 提出了<code>Xavier initialization</code>的初始化方式</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>这种初始化方式是从随机均匀分布初始化神经网络的，均匀分布的范围是</p>
<script type="math/tex; mode=display">
\pm \frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}</script><p>这里的 $n<em>i$ 是输入神经元数目，$n</em>{i+1}$ 是输出神经元数目。</p>
<p>Glorot and Bengio 认为Xavier 初始化方法，可以在包含激活函数的神经网络中保持方差的变化很小。</p>
<p><img src="https://tuchuang-1259359185.cos.ap-chengdu.myqcloud.com/bolgs/Xavier.png" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>除此之外，同样证明了，传统方法在底层网络方差大，高层网络方差趋近于0的现象。</p>
<p><img src="https://tuchuang-1259359185.cos.ap-chengdu.myqcloud.com/bolgs/Xavier2.png" srcset="/blog/img/loading.gif" lazyload alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">xavier</span>(<span class="hljs-params">m,n</span>):</span><br>    <span class="hljs-keyword">return</span> torch.Tensor(m,n).uniform_(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)/math.sqrt(<span class="hljs-number">6.</span>/(m+n))<br><br>x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = xavier(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x =  torch.tanh( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(0.0854) tensor(0.9933)</span><br>x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = xavier(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x =  torch.sigmoid( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment">#tensor(0.4686) tensor(0.4976)</span><br></code></pre></td></tr></table></figure>
<h3 id="Kaiming-Initialization"><a href="#Kaiming-Initialization" class="headerlink" title="Kaiming Initialization"></a>Kaiming Initialization</h3><p>进来CV领域中，激活方法多是采用<code>Relu</code> 函数。对于这个函数。之前的初始化方法，又有哪些不一样？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    x = torch.relu(a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment"># tensor(4.6656e-16) tensor(6.7154e-16)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = xavier(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x =  torch.relu( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment"># tensor(nan) tensor(nan)</span><br></code></pre></td></tr></table></figure>
<p>之前的初始化方法，对于<code>Relu</code>函数都不奏效了。那对于每一层来说，有什么变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">mean,var=<span class="hljs-number">0.</span>,<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    x = torch.randn(<span class="hljs-number">512</span>)<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>)<br>    y = torch.relu(a @ x)<br>    mean += y.mean().item()<br>    var += y.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean().item()<br><br><span class="hljs-built_in">print</span>(mean/<span class="hljs-number">10000</span>, math.sqrt(var/<span class="hljs-number">10000</span>))<br><span class="hljs-comment">#9.01142036409378 15.991211348807246</span><br><span class="hljs-built_in">print</span>(math.sqrt(<span class="hljs-number">512</span>/<span class="hljs-number">2</span>))<br><span class="hljs-comment">#16.0</span><br></code></pre></td></tr></table></figure>
<p>可以看到，这时候的输出跟输入网络层数大小是有关系的。在下面的实验验证以下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">mean,var=<span class="hljs-number">0.</span>,<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):<br>    x = torch.randn(<span class="hljs-number">512</span>)<br>    a = torch.randn(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)/math.sqrt(<span class="hljs-number">512</span>/<span class="hljs-number">2.</span>)<br>    y = torch.relu(a @ x)<br>    mean += y.mean().item()<br>    var += y.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean().item()<br><br><span class="hljs-built_in">print</span>(mean/<span class="hljs-number">10000</span>, math.sqrt(var/<span class="hljs-number">10000</span>))<br><span class="hljs-comment">#0.5640919140070677 1.0003173674661943</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kaiming</span>(<span class="hljs-params">m,n</span>):</span><br>    <span class="hljs-keyword">return</span> torch.randn(m,n)*math.sqrt(<span class="hljs-number">2.</span>/m)<br><br>x = torch.randn(<span class="hljs-number">512</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    a = kaiming(<span class="hljs-number">512</span>,<span class="hljs-number">512</span>)<br>    x = torch.relu( a @ x)<br><span class="hljs-built_in">print</span>(x.mean(), x.std())<br><span class="hljs-comment"># tensor(0.8135) tensor(1.2431)</span><br></code></pre></td></tr></table></figure>
<p>对照本部分开始的结果<code>kaiming</code>方法在对于<code>Relu</code>函数更有优势。</p>
<p>下图给出了两种方法在一个30层CNN上的结果。</p>
<p><img src="https://tuchuang-1259359185.cos.ap-chengdu.myqcloud.com/bolgs/kaiming.png" srcset="/blog/img/loading.gif" lazyload alt="kaiming method"></p>
<hr>
<p><strong>来源</strong>  <a target="_blank" rel="noopener" href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</a></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/blog/tags/DL/">DL</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2021/04/16/2018-09-20-normalization%20layer/">
                        <span class="hidden-mobile">BN及其变体</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/blog/js/debouncer.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/blog/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/blog/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/blog/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/blog/js/boot.js" ></script>


</body>
</html>
